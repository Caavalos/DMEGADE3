{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Maestría en Analítica de Negocios**\n",
        "##**Curso: Inteligencia Artificial y Aprendizaje Automático**\n",
        "###Tecnológico de Monterrey\n",
        "###Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "####**Filtrado de datos (data leakage) / Pipeline / Curvas de Aprendizaje**"
      ],
      "metadata": {
        "id": "iZH_9AwunsKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Librerías básicas que estaremos requiriendo en la mayoría de las actividades. \n",
        "# Recuerda usar el # para documentar tu código dentro de estas celdas de Código.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns   # para un mejor despliegue de los gráficos\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "WgYhkVetDbmh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Queremos accesar el archivo que está en la carpeta \"sample_data\" en la cual nos encontramos de manera  \n",
        "# predeterminada y que podemos verificar con el siguiente comando que nos permite listar sus archivos\n",
        "# y directorios:\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "Lm6QHJzj9WHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# La siguiente instrucción nos permite adentrarnos en dicha carpeta y de nuevo listamos lo que hay dentro de ella:\n",
        "\n",
        "%cd sample_data/\n",
        "\n",
        "!ls"
      ],
      "metadata": {
        "id": "Sq7zYLgoBcV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffesEiE3RAml"
      },
      "source": [
        "# En particular, los datos para el entrenamiento los encontramos obviamente en el siguiente archivo, el cual procedemos a cargar: \n",
        "\n",
        "data = pd.read_csv(\"california_housing_train.csv\", sep=\",\")\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "De cualquier de las ligas dadas anteriormente, sabemos que la variable de salida es el precio medio (mediana) de la casa en dólares estadounidenses, \"median_house_value\"."
      ],
      "metadata": {
        "id": "kkhimNeSexJZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygtqfXrjs_IG"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(12,10)})   # (ancho-columnas, altura-renglones) Ajustemos el tamaño de la ventana \n",
        "                                         # que desplegará los gráficos usando la librería de seaborn (sns)."
      ],
      "metadata": {
        "id": "xnaIL0diAHMr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(3, 3)    # definimos una ventana de 3x3 nichos para incluir en cada uno de ellos un gráfico.\n",
        "for k in range(0,9):\n",
        "  plt.subplot(3,3,k+1)     # los nichos para cada histograma se numeran iniciando en 1 y no en 0.\n",
        "  plt.hist(data[data.columns[k]], bins=20)     # datatrain.columns nos devuelve una lista con los nombres de las columnas.\n",
        "  plt.xlabel(data.columns[k])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XW3S8kW_qccK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "qrlfby2lIfAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Puedes nuevamente observar de esta tabla varias de las características que mencionamos previamente a partir de los histogramas."
      ],
      "metadata": {
        "id": "0BelKUNL3xtz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X1 =data[['total_bedrooms']]\n",
        "yy = data[['median_house_value']]"
      ],
      "metadata": {
        "id": "0r7lF8FUKAZ4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(6,4)})"
      ],
      "metadata": {
        "id": "Rajq4vWoKuww"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sqrt(X1).hist();"
      ],
      "metadata": {
        "id": "Do3IFv9YKjO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Modelo-1**"
      ],
      "metadata": {
        "id": "rNCF_CU4L0hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = LinearRegression()\n",
        "\n",
        "fit1 = m.fit(X1, yy)\n",
        "\n",
        "preds1 = fit1.predict(X1)\n",
        "preds1 = np.sqrt(mean_squared_error(yy, preds1))\n",
        "\n",
        "print(\"RMSE: %.2f\" % (preds1))"
      ],
      "metadata": {
        "id": "m5pSWXvvLMOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**¿Cuál es el error de diseño de este modelo, en caso de que exista, de acuerdo a la metodología de aprendizaje automático (machine learning)?**"
      ],
      "metadata": {
        "id": "U5hPi9VuMxVT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Modelo-2**"
      ],
      "metadata": {
        "id": "qJhF1U1iMtl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = LinearRegression()\n",
        "\n",
        "X2 = np.sqrt(X1)\n",
        "\n",
        "fit2 = m.fit(X2, yy)\n",
        "\n",
        "preds2 = fit2.predict(X2)\n",
        "preds2 = np.sqrt(mean_squared_error(yy, preds2))\n",
        "\n",
        "print(\"RMSE: %.2f\" % (preds2))"
      ],
      "metadata": {
        "id": "PWihUn1BLMLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**¿Cuál es el error de diseño de este modelo, en caso de que exista, de acuerdo a la metodología de aprendizaje automático (machine learning)?**"
      ],
      "metadata": {
        "id": "OUEuJYd6OJ9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Modelo-3**"
      ],
      "metadata": {
        "id": "x6iOJXktOLy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semilla = 11\n",
        "val_size = 0.2"
      ],
      "metadata": {
        "id": "FjFTtCo4yYYW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X2 = np.sqrt(X1)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X2, yy, test_size=val_size, random_state=semilla)\n",
        "\n",
        "\n",
        "\n",
        "m = LinearRegression()\n",
        "\n",
        "fit3 = m.fit(X_train, y_train)\n",
        "\n",
        "preds3 = fit3.predict(X_val)\n",
        "preds3 = np.sqrt(mean_squared_error(y_val, preds3))\n",
        "\n",
        "print(\"RMSE: %.2f\" % (preds3))"
      ],
      "metadata": {
        "id": "9XdQf-ouMT0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**¿Cuál es el error de diseño de este modelo, en caso de que exista, de acuerdo a la metodología de aprendizaje automático (machine learning)?**"
      ],
      "metadata": {
        "id": "vajHCpRzPySk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NOTA: Observa que al particionar el conjunto original de train en 80% y 20%, ya tenemos ahora el conjunto de entrenamiento y el de validación. El archivo restante que se encuentra en la carpeta del Google-Colab, llamado \"california_housing_test.csv\", será el conjunto de prueba, completando lo tres conjuntos con los cuales estaremos trabajando."
      ],
      "metadata": {
        "id": "50J4WlbvecCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Modelo-4**"
      ],
      "metadata": {
        "id": "IZqnyHmjPsC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X1, yy, test_size=val_size, random_state=semilla)\n",
        "\n",
        "X4_train = np.sqrt(X_train)\n",
        "X4_val = np.sqrt(X_val)\n",
        "\n",
        "\n",
        "\n",
        "m = LinearRegression()\n",
        "\n",
        "fit4 = m.fit(X4_train, y_train)\n",
        "\n",
        "preds4 = fit4.predict(X4_val)\n",
        "preds4 = np.sqrt(mean_squared_error(y_val, preds4))\n",
        "\n",
        "print(\"RMSE: %.2f\" % (preds4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucDMyuwZMTxq",
        "outputId": "05bbbcdb-8ef6-457e-b8f3-0eea976f21d4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 114709.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**¿Cuál es el error de diseño de este modelo, en caso de que exista, de acuerdo a la metodología de aprendizaje automático (machine learning)?**"
      ],
      "metadata": {
        "id": "aS9n2CYiQtGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Modelo-5**"
      ],
      "metadata": {
        "id": "Nke9Awnlgj4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supongamos que también deseamos escalar los datos mediante la transformación (x-miu)/std\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "yZZPxLYroILT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X1, yy, test_size=val_size, random_state=semilla)\n",
        "\n",
        "X5_train = np.sqrt(X_train)\n",
        "X5_train = (X5_train - np.mean(X5_train)) / np.std(X5_train)\n",
        "\n",
        "X5_val = np.sqrt(X_val)\n",
        "X5_val = (X5_val - np.mean(X5_val)) / np.std(X5_val)\n",
        "\n",
        "\n",
        "\n",
        "m = LinearRegression()\n",
        "\n",
        "fit5 = m.fit(X5_train, y_train)\n",
        "\n",
        "preds5 = fit5.predict(X5_val)\n",
        "preds5 = np.sqrt(mean_squared_error(y_val, preds5))\n",
        "\n",
        "print(\"RMSE: %.2f\" % (preds5))"
      ],
      "metadata": {
        "id": "C1AVr9OeoIIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**¿Cuál es el error de diseño de este modelo, en caso de que exista, de acuerdo a la metodología de aprendizaje automático (machine learning)?**"
      ],
      "metadata": {
        "id": "V70j-Sogi1GF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Modelo-6: Usando Pipelines para evitar el filtrado de información (data leakage)**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h0k2RjCVj9RZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Veamos a continuación la manera correcta de llevar a cabo las transformaciones mediante el uso de la clase Pipeline de scikit-learn. La clase Pipeline nos permitirá encapsular una serie de transformaciones para llevar a cabo el entrenamiento de un modelo, apoyando a evitar el filtrado de información."
      ],
      "metadata": {
        "id": "RUtu4mVOpAYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer   \n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "WOZlsPCYoICx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X1, yy, test_size=val_size, random_state=semilla)\n",
        "\n",
        "# Transformaciones a factores numéricos de entrada:\n",
        "num_pipeline = Pipeline(steps = [('impMediana', SimpleImputer(strategy='median')),\n",
        "                                 ('sqrt', FunctionTransformer(np.sqrt)),                                 \n",
        "                                 ('escalaNum', StandardScaler())])   \n",
        "\n",
        "num_pipeline_nombres = ['total_bedrooms']\n",
        "\n",
        "\n",
        "# Aplicamos dichas transformaciones a las variables indicadas y el resto las dejamos igual:\n",
        "columnasTransformer = ColumnTransformer(transformers = [('vars_num_pipeline', num_pipeline, num_pipeline_nombres)\n",
        "                                                        ],\n",
        "                                        remainder='passthrough')\n",
        "\n",
        "\n",
        "m = LinearRegression()\n",
        "\n",
        "# Aplicamos las transformaciones al conjunto de entrenamiento:\n",
        "XtrainFit = columnasTransformer.fit(X_train)   # Generamos la información necesaria con el conjunto de entrenamiento\n",
        "                                               # para evitar el filtrado de información.\n",
        "\n",
        "XtrainTransf = XtrainFit.transform(X_train)    # y ahora aplicamos dichas transformaciones al conjunto de entrenamiento,\n",
        "XvalTransf  =  XtrainFit.transform(X_val)      # y también al conjunto de validación (con la información de los datos de Train).\n",
        "\n",
        "\n",
        "fit6 = m.fit(XtrainTransf, y_train)\n",
        "\n",
        "preds6 = fit6.predict(XvalTransf)\n",
        "preds6 = np.sqrt(mean_squared_error(y_val, preds6))\n",
        "\n",
        "print(\"RMSE: %.2f\" % (preds6))"
      ],
      "metadata": {
        "id": "QrZ-wgwroH_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Curvas de Aprendizaje**"
      ],
      "metadata": {
        "id": "NuxSt-gU91AE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import RepeatedKFold"
      ],
      "metadata": {
        "id": "R3RF0k8k2jun"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modeloLC = LinearRegression()\n",
        "\n",
        "XtvTransf = XtrainFit.transform(X1)   # Como usaremos Cross-Validation, usamos el conjunto original de Train, X1 y sus valores reales yy.\n",
        "\n",
        "delta_train_sz = np.linspace(0.1, 1.0, 20)\n",
        "\n",
        "cvLC = RepeatedKFold(n_splits=10, n_repeats=5, random_state=semilla)\n",
        "\n",
        "train_sizes, train_scores, valid_scores = learning_curve(estimator=modeloLC, \n",
        "                                                        X=XtvTransf, \n",
        "                                                        y=yy,\n",
        "                                                        cv=cvLC, \n",
        "                                                        train_sizes=delta_train_sz,\n",
        "                                                        scoring='neg_root_mean_squared_error')\n",
        "\n",
        "\n",
        "\n",
        "# Obtengamos la gráfica de las curvas de aprendizaje \n",
        "# cuando se incrementa el tamaño de la muestra:\n",
        "\n",
        "train_mean = -np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "valid_mean = -np.mean(valid_scores, axis=1)\n",
        "valid_std = np.std(valid_scores, axis=1)\n",
        "\n",
        "plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training')\n",
        "plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
        "\n",
        "plt.plot(train_sizes, valid_mean, color='red', marker='o', markersize=5, label='Validation')\n",
        "plt.fill_between(train_sizes, valid_mean + valid_std, valid_mean - valid_std, alpha=0.15, color='red')\n",
        "\n",
        "plt.title('Función learning_curve()')\n",
        "plt.xlabel('Tamaño del conjunto de entrenamiento')\n",
        "plt.ylabel('RMSE')\n",
        "plt.grid()\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WJggNLrS2uHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NOTA: En el caso de scikit-learn, recuerda que calcula el valor negativo de RMSE.\n",
        "\n",
        "### Consulta la documentación correspondiente:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n"
      ],
      "metadata": {
        "id": "rW6vmZoy_Jzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Modelo final con el conjunto de Prueba (Test)**"
      ],
      "metadata": {
        "id": "h52Mky9_pihG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datatest = pd.read_csv(\"california_housing_test.csv\", sep=\",\")   # cargamos los datos de la carpeta del Google-Colab, observa que\n",
        "                                                                 # hasta este momento no se habían usado estos datos.\n",
        "datatest.shape"
      ],
      "metadata": {
        "id": "1qZmU4nAmEKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtest =datatest[['total_bedrooms']]\n",
        "ytest = datatest[['median_house_value']]"
      ],
      "metadata": {
        "id": "f0Pte8hNp4fA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mf = LinearRegression()\n",
        "\n",
        "XtvTransf = XtrainFit.transform(X1)   # Observa que al obtener el entrenamiento final, usamos los datos tanto de Train como de Validation X1,\n",
        "                                      # esto permitirá todavía usar una mayor cantidad de información para aprender mejor. Sin embargo,\n",
        "                                      # seguimos usando la información solo del conjunto Train para evitar el filtrado de información.\n",
        "\n",
        "XtestTransf  =  XtrainFit.transform(Xtest)  \n",
        "\n",
        "fitf = mf.fit(XtvTransf, yy)\n",
        "\n",
        "predsf = fitf.predict(XtestTransf)\n",
        "predsf = np.sqrt(mean_squared_error(ytest, predsf))\n",
        "\n",
        "print(\"RMSE: %.2f\" % (predsf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtsL3V60qKop",
        "outputId": "e50d8e6f-d7a9-4ca5-f3b4-3b35d81714b3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 112733.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\tModelo1\\t\\tModelo2\\t\\tModelo3\\t\\tModelo4\\t\\tModelo5\\t\\tModelo6\\t\\tModeloF\")\n",
        "\n",
        "print(\"RMSE:  %.2f\\t%.2f\\t%.2f\\t%.2f\\t%.2f\\t%.2f\\t%.2f\\t\" % (preds1,preds2,preds3,preds4,preds5,preds6,predsf) )\n",
        "\n"
      ],
      "metadata": {
        "id": "Iibl93BUu6Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Conclusiones ..."
      ],
      "metadata": {
        "id": "Ip2Jkr2bV98C"
      }
    }
  ]
}